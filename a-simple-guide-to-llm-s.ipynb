{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports, Keep in mind different LLMs are trained in different versions of PyTorch and other libraries, so it is very common to have a version mismatch. Must read documentation and GitHub or HF issues.","metadata":{"execution":{"iopub.status.busy":"2025-08-08T20:16:57.507378Z","iopub.execute_input":"2025-08-08T20:16:57.508011Z","iopub.status.idle":"2025-08-08T20:16:57.511619Z","shell.execute_reply.started":"2025-08-08T20:16:57.507983Z","shell.execute_reply":"2025-08-08T20:16:57.510858Z"}}},{"cell_type":"code","source":"!pip install  -q transformers torch accelerate bitsandbytes peft trl datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Model from HF and performing inference.","metadata":{}},{"cell_type":"code","source":"    \"\"\"\n    Imports the AutoModelForCausalLM and AutoTokenizer classes from the\n    Hugging Face transformers library. These classes allow for easy loading\n    of pre-trained models and their corresponding tokenizers.\n    \"\"\"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\n    \"\"\"\n    Downloads and loads the tokenizer that is specifically associated with the\n    chosen model. The tokenizer is responsible for converting text into\n    numerical tokens and vice versa.\n    \"\"\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n    \"\"\"\n    Downloads and loads the actual pre-trained language model.\n    - model_name: Specifies which model to load.\n    - torch_dtype=\"auto\": Allows the library to automatically select the optimal\n      data type (like float16) for memory efficiency.\n    - device_map=\"auto\": Automatically moves the model to the appropriate hardware,\n      such as a GPU if one is available, for faster processing.\n    \"\"\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to a large language model.\"\n\n    \n    \"\"\"\n    Formats the prompt into a list of dictionaries, following the conversational\n    chat structure that this model expects. Each dictionary represents one turn\n    in the conversation, with a 'role' (like 'user') and 'content'.\n    \"\"\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\n    \"\"\"\n    Applies the model's specific chat template to the 'messages' list. This\n    converts the structured conversation into a single, formatted string,\n    adding all necessary special tokens (like '<|im_start|>') that the\n    model needs to understand the prompt correctly.\n    - tokenize=False: Ensures the output is a string, not yet token IDs.\n    - add_generation_prompt=True: Appends a marker at the end to signal\n      the model that it's now its turn to generate a response.\n    \"\"\"\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n    \"\"\"\n    Performs the final tokenization step. It takes the formatted string,\n    converts it into numerical token IDs, wraps it in a list to create a batch\n    of one, and returns the result as PyTorch tensors. The '.to(model.device)'\n    call moves these tensors to the same hardware (CPU or GPU) as the model.\n    \"\"\"\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    \"\"\"\n    Executes the core text generation process. The model takes the tokenized\n    inputs and generates a sequence of new token IDs as the response.\n    - **model_inputs: Unpacks the dictionary of tensors into keyword arguments.\n    - max_new_tokens: Sets a very high limit on the length of the generated\n      response to ensure it isn't cut off prematurely.\n    \"\"\"\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\n    \"\"\"\n    Processes the output from the model. It selects the first (and only) sequence\n    from the batch, slices it to remove the original input prompt tokens, and\n    keeps only the newly generated tokens. These token IDs are then converted\n    into a standard Python list.\n    \"\"\"\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n    \"\"\"\n    Starts a 'try...except' block. This is a safe way to handle the parsing\n    logic, as it will prevent the code from crashing if the special\n    '</think>' token is not found in the model's output.\n    \"\"\"\ntry:\n    \"\"\"\n    Finds the position of the special '</think>' token (ID 151668), which\n    separates the model's reasoning from its final answer. It works by\n    reversing the list of output IDs, finding the first index of the token,\n    and then calculating its original position from the end of the list.\n    \"\"\"\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\n\"\"\"\nThis 'except' block catches the 'ValueError' that occurs if the '.index()'\nmethod cannot find the token ID 151668 in the list.\n\"\"\"\nexcept ValueError:\n    \"\"\"\n    If the '</think>' token is not found, the index is set to 0. This means\n    The entire output will be treated as the final answer, with no\n    \"thinking\" content to extract.\n    \"\"\"\n    index = 0\n\n    \"\"\"\n    Decodes the list of token IDs that appear *before* the '</think>' token\n    into a human-readable string. This string represents the model's internal\n    \"thinking\" or reasoning process.\n    \"\"\"\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n    \"\"\"\n    Decodes the list of token IDs from the '</think>' token to the end of the\n    sequence. This string represents the final, user-facing answer.\n    \"\"\"\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:08:56.540256Z","iopub.execute_input":"2025-08-08T20:08:56.540978Z","iopub.status.idle":"2025-08-08T20:09:59.085543Z","shell.execute_reply.started":"2025-08-08T20:08:56.540951Z","shell.execute_reply":"2025-08-08T20:09:59.084714Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a55c7868c14895bd2f7fd12d2a0ea7"}},"metadata":{}},{"name":"stdout","text":"thinking content: Okay, the user asked for a short introduction to large language models. Let me start by recalling what I know about LLMs. They're AI models trained on massive amounts of text data. I should mention their key features like understanding and generating human-like text.\n\nHmm, the user probably wants something concise but informative. They might be a student, a professional new to AI, or just someone curious. I should avoid jargon to keep it accessible. Maybe they need this for a project, a presentation, or personal knowledge. \n\nWait, they said \"short,\" so I need to be brief. Focus on the essentials: definition, training data, capabilities, and examples. Don't dive too deep into technical details like architecture or training processes. \n\nI should highlight what LLMs can do—answer questions, write stories, code—but also mention limitations to set realistic expectations. The user might not know that LLMs can make mistakes or be biased. Including a note about context and training data could help them understand why outputs vary.\n\nOh, right, the term \"large\" refers to the scale—billions of parameters. That's important to mention but explain simply. Maybe say \"massive amounts of text\" instead of diving into parameters. \n\nExamples like GPT-3, Gemini, Claude would make it relatable. The user might have heard of these names before. \n\nAlso, should I mention applications? Briefly, like chatbots, content creation, coding assistants. That shows real-world use. \n\nCheck if the response flows: start with a simple definition, then how they work (training on text), what they do, examples, and a quick note on limitations. Keep it under 100 words? Wait, the user said \"short,\" so maybe 3-4 sentences. \n\nWait, the previous response was 4 sentences. Let me count: \"Large language models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like language. They learn patterns and meanings from this data, enabling tasks like answering questions, writing stories, coding, and more. Examples include GPT-3, Gemini, and Claude. While powerful, they can make mistakes or reflect biases in their training data.\" That's 4 sentences. Good. \n\nIs there anything missing? Maybe the \"why large\" part—scale matters for performance. But the user wants short, so maybe skip that. The term \"large\" is covered by \"vast amounts of text data.\" \n\nAlso, the user might confuse LLMs with other AI models. Clarifying they're specifically for language tasks is key. \n\nI think that covers it. Keep it neutral, factual, and concise. Avoid markdown as per instructions. Alright, this should work.\n</think>\ncontent: Here's a concise introduction to large language models (LLMs):\n\n> **Large language models (LLMs)** are advanced AI systems trained on massive amounts of text data (like books, websites, and documents) to understand and generate human-like language. By learning patterns and meanings from this data, they can answer questions, write stories, code, create emails, and perform other language tasks. Examples include models like GPT-3, Gemini, and Claude. While powerful, their outputs depend on training data and can sometimes contain errors or biases.\n\n*(~75 words)*\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Printing the Model Architecture.","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:10:26.084415Z","iopub.execute_input":"2025-08-08T20:10:26.084715Z","iopub.status.idle":"2025-08-08T20:10:26.091337Z","shell.execute_reply.started":"2025-08-08T20:10:26.084691Z","shell.execute_reply":"2025-08-08T20:10:26.090437Z"}},"outputs":[{"name":"stdout","text":"Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 2560)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Batch Processing of Prompts","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\"\"\"\nDownloads and loads the actual pre-trained language model weights.\n- torch_dtype=\"auto\": Automatically selects the best data precision for efficiency.\n- device_map=\"auto\": Automatically moves the model to the GPU if available.\n\"\"\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n\"\"\"\nThis block checks if the tokenizer has a pre-defined padding token.\nPadding is necessary when processing multiple texts of different lengths\nat the same time (i.e., in a batch).\n\"\"\"\n# === BATCHING CHANGE 1: Set a padding token ===\n# The model needs a token to use for padding shorter sequences in the batch.\n# The end-of-sequence token is a common choice for this.\nif tokenizer.pad_token is None:\n    \"\"\"\n    If no padding token is defined, this line sets the existing end-of-sequence\n    (eos) The token is also to be used as the padding token. This is a common practice.\n    \"\"\"\n    tokenizer.pad_token = tokenizer.eos_token\n\n\"\"\"\nThis creates a Python list containing multiple different prompts. This list\nrepresents the batch of inputs that will be sent to the model simultaneously.\n\"\"\"\nprompts = [\n    \"Give me a short introduction to large language models.\",\n    \"What are the capital cities of Bangladesh and Japan?\",\n    \"Write a two-sentence horror story.\"\n]\n\n\"\"\"\nThis uses a list comprehension to iterate through each prompt in the 'prompts'\nList and format it into the required conversational dictionary structure.\n\"\"\"\nmessages = [\n    [{\"role\": \"user\", \"content\": p}] for p in prompts\n]\n\"\"\"\nThis uses another list comprehension to apply the model's chat template to\neach conversational message, creating a list of fully formatted strings.\n\"\"\"\ntexts = [\n    tokenizer.apply_chat_template(\n        m,\n        tokenize=False,\n        add_generation_prompt=True,\n    ) for m in messages\n]\n\n\"\"\"\nThis tokenizes the entire batch of formatted text strings at once.\n- padding=True: Adds padding to shorter sentences so all sequences in the\n  batch have the same length.\n- return_tensors=\"pt\": Returns the output as PyTorch tensors.\n- .to(model.device): Moves the tensors to the same hardware (GPU or CPU) as the model.\n\"\"\"\nmodel_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(model.device)\n\n\"\"\"\nThis executes text generation for the entire batch. The model processes all\nThe input prompts and generates a response for each one.\n\"\"\"\n# conduct text completion for the batch\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512 # Reduced for faster batch processing\n)\n\n\"\"\"\nThis starts a 'for' loop to iterate through each of the generated responses\nin the output batch, so we can process them one by one.\n\"\"\"\nfor i in range(len(prompts)):\n\n    # Get the generated tokens for the i-th prompt, excluding the original input tokens\n    input_len = len(model_inputs.input_ids[i])\n\n    output_ids = generated_ids[i][input_len:].tolist()\n\n\n  \n    try:\n        # rindex finding 151668 (</think>)\n        index = len(output_ids) - output_ids[::-1].index(151668)\n    \"\"\"\n    This 'except' block will run if the '</think>' token ID is not found.\n    \"\"\"\n    except ValueError:\n\n        index = 0\n\n    \"\"\"\n    This decodes the token IDs *before* the '</think>' token into a string,\n    representing the model's reasoning.\n    \"\"\"\n    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n    \"\"\"\n    This decodes the token IDs from the '</think>' token onward into a string,\n    representing the final answer.\n    \"\"\"\n    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\n    print(\"=\"*50)\n    print(f\"PROMPT {i+1}: {prompts[i]}\")\n    print(\"-\" * 20)\n    print(\"Thinking Content:\", thinking_content)\n    print(\"Content:\", content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:10:31.611615Z","iopub.execute_input":"2025-08-08T20:10:31.611909Z","iopub.status.idle":"2025-08-08T20:12:49.198877Z","shell.execute_reply.started":"2025-08-08T20:10:31.611888Z","shell.execute_reply":"2025-08-08T20:12:49.198099Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2944bca90c264700bde10f82cc5f88a9"}},"metadata":{}},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nPROMPT 1: Give me a short introduction to large language models.\n--------------------\nThinking Content: \nContent: Okay, the user asked for a short introduction to large language models. Hmm, they probably want something concise but informative—no jargon overload. Maybe they're a student, a curious professional, or just someone who heard the term and wants a quick grasp.  \n\nFirst, I should clarify what LLMs *are* without diving too deep into technicalities. The key points: massive parameters, training on text, pattern recognition, and generation. Gotta emphasize they're not \"intelligent\" but statistically powerful—they predict text based on patterns, not true understanding.  \n\nWait, should I mention specific examples like GPT or Llama? Yeah, naming a couple would make it concrete. But keep it brief—user said \"short.\" Also, avoid hype; don't want to imply they have consciousness or reasoning. Important distinction!  \n\nUser might confuse LLMs with chatbots. Should I clarify that LLMs *power* chatbots but aren't the chatbots themselves? Maybe in the intro, but since they asked for \"short,\" I'll skip that unless it's critical.  \n\nBiggest pitfall: oversimplifying. Like saying \"they learn human language\"—technically true, but they learn *statistical patterns* from data, not language itself. Gotta phrase it carefully.  \n\nAlso, the user might wonder *why* they matter. So I'll add a line about real-world uses (writing, coding, etc.) to show relevance without getting technical.  \n\n*Structure thoughts*:  \n1. Definition (what they are)  \n2. How they work (training, patterns)  \n3. What they do (generate text)  \n4. Key traits (scale, not \"smart\")  \n5. Real-world context (brief examples)  \n\nKeep it under 100 words? User said \"short,\" so aim for 3-4 sentences max.  \n\n*Double-check*: No markdown, plain English. Avoid terms like \"transformers\" unless necessary—too technical for intro. Say \"neural networks\" instead? Maybe, but \"neural networks\" might confuse non-techies. Stick to \"statistical patterns\" and \"training on vast text.\"  \n\nFinal phrasing: Focus on simplicity. \"Large language models (LLMs) are AI systems trained on massive amounts of text...\" Yep, that starts strong.  \n\n*Self-reminder*: Don't say they \"understand\" language—they don't. Emphasize prediction vs. comprehension. User might have misconceptions here.\n==================================================\nPROMPT 2: What are the capital cities of Bangladesh and Japan?\n--------------------\nThinking Content: Okay, the user is asking about the capital cities of Bangladesh and Japan. Let me start by recalling the correct information. I remember that Dhaka is the capital of Bangladesh. For Japan, it's Tokyo. But I should double-check to be sure.\n\nHmm, why might the user need this? They could be a student working on a geography project, a traveler planning a trip, or someone preparing for a quiz. Knowing the context might help provide more useful info. But since they didn't specify, I'll stick to the basics.\n\nWait, is there any chance of confusion? Sometimes people mix up country capitals. For example, Bangladesh's capital is Dhaka, not Chittagong. And Japan's capital is Tokyo, not Osaka. I should make sure there are no recent changes. I recall that Japan's capital has always been Tokyo since 1868, and Bangladesh's capital has been Dhaka since independence. No recent changes there.\n\nAlso, the user might not know that Bangladesh was formerly known as East Pakistan. But that's probably extra. Just the capitals should suffice. Maybe mention that Dhaka is the largest city in Bangladesh to add context. For Japan, Tokyo is the most populous city, which might be relevant.\n\nI should present the answer clearly: Bangladesh's capital is Dhaka, Japan's is Tokyo. Keep it concise. Avoid markdown as per the instructions. Use simple language. \n\nWait, the user might be non-native English speakers. So using straightforward terms without jargon. No need for extra details unless necessary. But adding a bit about Dhaka being the administrative center could help. For Japan, noting that Tokyo is the political and economic hub might be useful.\n\nDouble-check sources quickly in my mind. Reliable sources like official government websites or well-known encyclopedias confirm Dhaka and Tokyo. No doubts there. \n\nAlso, consider if the user meant something else. Like, are there any other cities claimed as capitals? But no, Dhaka and Tokyo are universally accepted. \n\nSo the answer is straightforward. Just state both capitals clearly. Maybe add a sentence about each to provide context without overwhelming the user. \n\nFinal answer: Bangladesh's capital is Dhaka, Japan's capital is Tokyo.\n</think>\nContent: The capital cities are:\n\n- **Bangladesh**: **Dhaka** (the largest city and administrative center of the country).\n- **Japan**: **Tokyo** (the most populous city and the seat of the Japanese government).\n\nBoth are well-established and officially recognized capitals with no recent\n==================================================\nPROMPT 3: Write a two-sentence horror story.\n--------------------\nThinking Content: <think>\nOkay, the user asked for a two-sentence horror story. Hmm, they probably want something concise but impactful—horror thrives on brevity and psychological dread. \n\nI should avoid clichés like jump scares or gore since two sentences leave little room for elaboration. Maybe focus on subtle, unsettling details that imply something wrong without stating it outright. \n\nThe user might be testing my ability to create tension efficiently—perhaps they're a writer looking for inspiration, or just someone wanting a quick scare. Either way, the story needs to linger in the mind. \n\nI'll go with a mundane scenario turned horrifying: a child's drawing. Kids' art is innocent, so the twist—something *not* human in the drawing—creates immediate unease. The phrase \"it wasn't a drawing\" implies the child is already affected, which feels more chilling than direct violence. \n\n*Checks sentence count*: First sentence sets up normalcy, second reveals the horror. Nailed it.\n</think>\nContent: The child drew a perfect, detailed picture of a house with a window wide open. When I looked closer, the figure inside wasn't a person—it was the exact shape of the empty space where the child's hand had been holding the pencil.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:13:01.425376Z","iopub.execute_input":"2025-08-08T20:13:01.426005Z","iopub.status.idle":"2025-08-08T20:13:01.430169Z","shell.execute_reply.started":"2025-08-08T20:13:01.425980Z","shell.execute_reply":"2025-08-08T20:13:01.429405Z"}},"outputs":[{"name":"stdout","text":"['<|im_start|>user\\nGive me a short introduction to large language models.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n', '<|im_start|>user\\nWhat are the capital cities of Bangladesh and Japan?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n', '<|im_start|>user\\nWrite a two-sentence horror story.<|im_end|>\\n<|im_start|>assistant\\n<think>\\n']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Playing with Tokenizer. ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# You can use any model from the Hugging Face Hub.\n# Let's use a well-known, smaller model for this example.\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\nprint(f\"Loading tokenizer for model: '{model_name}'\\n\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# --- Let's start with a sample sentence ---\nsentence = \"Transformers are very powerful tools for NLP.\"\nprint(\"Original Sentence:\", sentence)\nprint(\"=\"*50)\n\n\n# 1. TOKENIZE: Convert text into tokens (strings)\n# This shows how the tokenizer breaks up the words.\ntokens = tokenizer.tokenize(sentence)\nprint(\"1. Text split into Tokens (strings):\")\nprint(tokens)\nprint(\"-\"*50)\n\n\n# 2. ENCODE: Convert text into token IDs (numbers)\n# This is what the model actually receives as input.\ntoken_ids = tokenizer.encode(sentence)\nprint(\"2. Text converted into Token IDs (numbers):\")\nprint(token_ids)\nprint(\"-\"*50)\n\n\n# 3. DECODE: Convert token IDs back into text\n# This shows that the process is reversible.\ndecoded_text = tokenizer.decode(token_ids)\nprint(\"3. Token IDs converted back into Text:\")\nprint(decoded_text)\nprint(\"=\"*50)\n\n\n# 4. EXPLORE SPECIAL TOKENS\n# Models use special tokens to understand the structure of the input.\nprint(\"4. Exploring Special Tokens:\")\n\n# Get the entire map of special tokens\nspecial_tokens_map = tokenizer.special_tokens_map\nprint(\"\\n   - Map of all special tokens:\")\nprint(special_tokens_map)\n\n# Get specific tokens and their numerical IDs\ncls_token = tokenizer.cls_token\ncls_token_id = tokenizer.cls_token_id\nsep_token = tokenizer.sep_token\nsep_token_id = tokenizer.sep_token_id\n\nprint(f\"\\n   - Classification Token: '{cls_token}' (ID: {cls_token_id})\")\nprint(f\"   - Separator Token:    '{sep_token}' (ID: {sep_token_id})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:13:02.509492Z","iopub.execute_input":"2025-08-08T20:13:02.510193Z","iopub.status.idle":"2025-08-08T20:13:03.156410Z","shell.execute_reply.started":"2025-08-08T20:13:02.510168Z","shell.execute_reply":"2025-08-08T20:13:03.155515Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer for model: 'Qwen/Qwen3-4B-Thinking-2507'\n\nOriginal Sentence: Transformers are very powerful tools for NLP.\n==================================================\n1. Text split into Tokens (strings):\n['Transform', 'ers', 'Ġare', 'Ġvery', 'Ġpowerful', 'Ġtools', 'Ġfor', 'ĠN', 'LP', '.']\n--------------------------------------------------\n2. Text converted into Token IDs (numbers):\n[8963, 388, 525, 1602, 7988, 7375, 369, 451, 12567, 13]\n--------------------------------------------------\n3. Token IDs converted back into Text:\nTransformers are very powerful tools for NLP.\n==================================================\n4. Exploring Special Tokens:\n\n   - Map of all special tokens:\n{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n\n   - Classification Token: 'None' (ID: None)\n   - Separator Token:    'None' (ID: None)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"\n# Loading a Quantized LLM for Efficient Inference\n\nThis script demonstrates how to load a powerful Large Language Model (`Qwen/Qwen3-4B-Thinking-2507`) in a highly memory-efficient way using **4-bit quantization**. This technique is essential for running large models on consumer-grade hardware (like gaming GPUs) by significantly reducing the model's size in memory (VRAM).\n\nThe process involves three main steps:\n1.  **Defining the Quantization Configuration:** We specify *how* the model's precision should be reduced.\n2.  **Loading the Model:** We load the model from the Hugging Face Hub, applying the specified configuration.\n3.  **Running Inference:** We test the quantized model with a sample prompt to ensure it works as expected.\n\n---\n### Step 1: Quantization Configuration ⚙️\nBefore loading the model, we create a `BitsAndBytesConfig` object. This object acts as a set of instructions for the `transformers` library on how to compress the model's weights.\n* **`load_in_4bit=True`**: This is the main command that enables 4-bit quantization.\n* **`bnb_4bit_quant_type=\"nf4\"`**: This specifies the quantization method. \"nf4\" (Normalized Float 4) is a highly effective and popular technique that maintains good model performance.\n* **`bnb_4bit_compute_dtype=torch.bfloat16`**: While the model's weights are *stored* in 4-bit, the actual mathematical calculations during inference are performed in a more stable, higher-precision format (`bfloat16`) to maintain accuracy. Think of it like decompressing a small section of a JPEG image to full quality right before you view it.\n\n---\n### Step 2: Loading the Quantized Model 🧠\nWe use the standard `AutoModelForCausalLM.from_pretrained` function but pass our `quantization_config` to it. The `transformers` library, in conjunction with `bitsandbytes`, handles the complex process of downloading the model and compressing its weights on the fly. The `device_map=\"auto\"` argument intelligently places the model on your GPU for fast performance.\n\n---\n### Step 3: Performing Inference ▶️\nAfter the model is loaded, we can interact with it just like any other language model. The script prepares a sample prompt (\"What is quantization in the context of LLMs?\"), formats it using the model's specific chat template, tokenizes it, and then calls `model.generate()` to get a response. This final step confirms that our memory-efficient model is fully functional and ready to use.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\n# 1. Define the Quantization Configuration\n# This tells the model how to compress its weights.\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# 2. Load the model with the quantization config\nprint(\"Loading quantized model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\", # Automatically uses the GPU\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"\\nModel loaded successfully in 4-bit!\")\n\n# You can now use the model for inference just like before.\n# It will consume much less VRAM.\n# For example:\nprompt = \"What is quantization in the context of LLMs?\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=100)\nresponse = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nprint(\"\\n--- Sample Inference ---\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:13:11.490040Z","iopub.execute_input":"2025-08-08T20:13:11.490341Z","iopub.status.idle":"2025-08-08T20:13:32.786668Z","shell.execute_reply.started":"2025-08-08T20:13:11.490319Z","shell.execute_reply":"2025-08-08T20:13:32.785785Z"}},"outputs":[{"name":"stdout","text":"Loading quantized model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9615e47830c7432589327529b0ecf097"}},"metadata":{}},{"name":"stdout","text":"\nModel loaded successfully in 4-bit!\n\n--- Sample Inference ---\nuser\nWhat is quantization in the context of LLMs?\nassistant\n<think>\nOkay, the user is asking about quantization in the context of LLMs. Let me start by recalling what I know. Quantization is a technique to reduce the size and computational requirements of models. But I need to make sure I cover all aspects clearly.\n\nFirst, I should explain why quantization is needed for LLMs. Large language models have massive parameters, often in the billions or trillions. Training them requires huge memory and powerful GPUs. For deployment, especially on edge devices like\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"\n# Setting up a Model for QLoRA Fine-Tuning\n\nThis script demonstrates the essential setup process for **QLoRA (Quantized Low-Rank Adaptation)**, a memory-efficient technique for fine-tuning large language models. The goal is to load a large base model in a compressed format and then insert small, trainable \"adapter\" layers. The script concludes by verifying the setup, printing a comparison between the total number of parameters in the model and the tiny fraction that will actually be trained.\n\nThe process is broken down into three parts: configuring the quantization, applying the LoRA adapters, and finally, printing the trainable parameter count.\n\n---\n### 1. The 'Q' in QLoRA: Quantization with `BitsAndBytesConfig`\n\nThe first step is to load the base model in a highly compressed state to save GPU memory (VRAM). We use a `BitsAndBytesConfig` object to give precise instructions on how to perform this compression.\n\n* **`load_in_4bit=True`**: This is the master switch that enables 4-bit quantization.\n* **`bnb_4bit_quant_type=\"nf4\"`**: This specifies the use of **Normalized Float 4 (nf4)**, a special data type optimized for the typical distribution of weights in a neural network, which helps preserve model performance.\n* **`bnb_4bit_compute_dtype=torch.bfloat16`**: This is a crucial setting for performance. While the model's weights are *stored* in memory-saving 4-bit, the actual mathematical calculations are performed in the more stable 16-bit `bfloat16` format. The model dynamically de-quantizes only the necessary weights for each calculation, ensuring accuracy without sacrificing the memory benefits.\n* **`bnb_4bit_use_double_quant=True`**: This enables a second quantization pass that compresses the metadata from the first pass, saving a little extra memory.\n\nThe model is then loaded using `AutoModelForCausalLM.from_pretrained`, with our `quantization_config` applied. The base model's weights are now frozen in a 4-bit state on the GPU.\n\n---\n### 2. The 'LoRA' in QLoRA: Low-Rank Adaptation\n\nWith the base model frozen, we need to add new parameters that we can train. This is done using LoRA.\n\n* **`prepare_model_for_kbit_training`**: This is a utility function that prepares the quantized model by setting up gradient checkpointing and ensuring compatibility for training.\n* **`LoraConfig`**: This object defines the properties of the small, trainable adapter layers we will inject into the model.\n    * **`target_modules=[...]`**: This tells the library exactly which layers of the frozen model to wrap with adapters. We typically target the attention mechanism's projection layers (`q_proj`, `k_proj`, etc.), as they are critical to the model's behavior.\n    * **`r=16`**: This is the **rank** of the adapter, which controls its size. A smaller `r` means fewer trainable parameters.\n    * **`lora_alpha=32`**: This is a scaling factor for the updates applied by the adapter.\n* **`get_peft_model()`**: This function takes the base model and the LoRA configuration and returns the final model, now ready for training. The original weights are still frozen, but the new LoRA adapter weights are trainable.\n\n---\n### 3. Verifying the Setup\n\nThe `print_trainable_parameters` function is a crucial final check. It iterates through all the parameters in the modified model and counts how many are \"trainable\" (i.e., `param.requires_grad` is `True`).\n\nThe output clearly demonstrates the efficiency of QLoRA by showing that the number of trainable parameters is only a tiny fraction (often less than 1%) of the total parameters. This confirms that our fine-tuning process will be extremely memory-efficient.","metadata":{}},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Model to fine-tune\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || \"\n        f\"trainable%: {100 * trainable_params / all_param:.2f}\"\n    )\n\n# Call the function to see the result\nprint_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:35:57.116967Z","iopub.execute_input":"2025-08-08T20:35:57.117286Z","iopub.status.idle":"2025-08-08T20:36:09.798804Z","shell.execute_reply.started":"2025-08-08T20:35:57.117262Z","shell.execute_reply":"2025-08-08T20:36:09.797996Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4532ee8f58b74a5ab42f01737cfb452a"}},"metadata":{}},{"name":"stdout","text":"trainable params: 11796480 || all params: 2217606656 || trainable%: 0.53\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# A Detailed Guide to Fine-tuning with QLoRA\n\nThis script fine-tunes the `Qwen/Qwen3-4B-Thinking-2507` model, teaching it to adopt a new personality using the **QLoRA** method. QLoRA is a breakthrough technique for making large language models (LLMs) accessible for fine-tuning on consumer-grade hardware. It achieves this by being incredibly memory-efficient.\n\nThe entire process can be broken down into four key stages: preparing the data, configuring the model with QLoRA, setting up the trainer, and finally, training and saving the result.\n\n---\n### 1. Data Preparation 📚\nBefore we can teach the model a new skill, we need to create the learning material.\n\n* **Create Raw Data:** We begin with basic Python lists for our `prompts` and the desired `responses`. This format is intuitive and easy to manage.\n* **Structure the Dataset:** We convert these lists into a Hugging Face `Dataset` object. This is a standardized data structure that integrates seamlessly with the entire Hugging Face ecosystem.\n* **Apply Chat Template:** We define a `formatting_prompts_func` to structure our prompts and responses into the specific chat format the Qwen model was originally trained on. Using the `.map()` function, we apply this template to every example, ensuring the model receives input in a format it understands, complete with special tokens like `<|im_start|>user`.\n* **Tokenize the Dataset:** Finally, we convert our formatted text into numbers via tokenization. The `tokenize_function` not only converts text to token IDs but also handles two critical tasks for batching:\n    * **Truncation:** It cuts down any examples longer than `max_length` (1024 tokens).\n    * **Padding:** It adds special `pad_token`s to any examples shorter than `max_length`, ensuring every sequence in a batch has the exact same length.\n\n---\n### 2. The QLoRA Configuration ⚙️\nThis is the heart of the process, where we combine Quantization and Low-Rank Adaptation.\n\n#### The 'Q' in QLoRA: Quantization with `BitsAndBytesConfig`\nThe primary bottleneck in running LLMs is GPU memory (VRAM). A 4-billion-parameter model would normally require `4B * 2 bytes/param = 8 GB` of VRAM just for the model weights in 16-bit precision, plus more for gradients and optimizer states during training. Quantization drastically reduces this. We configure it using the `BitsAndBytesConfig` object, which acts as a detailed instruction manual for this compression.\n\n* **`load_in_4bit=True`**: This is the master switch that enables 4-bit quantization. Instead of loading the model's weights as 16-bit numbers, this tells the library to apply a sophisticated 4-bit compression scheme as the model is loaded onto the GPU.\n* **`bnb_4bit_quant_type=\"nf4\"`**: This specifies the *type* of 4-bit numbers to use. `\"nf4\"` stands for **Normalized Float 4**. Unlike a standard 4-bit integer, nf4 is a special data type designed by researchers to be optimal for data that follows a normal (bell-curve) distribution, which is exactly how weights in a neural network are typically distributed. This clever design minimizes the loss of information, preserving the model's performance remarkably well.\n* **`bnb_4bit_compute_dtype=torch.bfloat16`**: This is a crucial concept for understanding QLoRA's efficiency. The model's weights are *stored* in memory-saving 4-bit. However, to perform actual calculations (like matrix multiplications), 4-bit precision is not enough and would lead to poor performance. This parameter tells the library to use `bfloat16` as the **computation data type**. When the model needs to perform a calculation, it takes the tiny chunk of 4-bit weights required, dynamically \"de-quantizes\" it up to the more precise `bfloat16` format, performs the math, and then immediately discards the `bfloat16` version, keeping only the 4-bit weights in memory. This gives you the best of both worlds: the low memory footprint of 4-bit storage and the accuracy of 16-bit computation.\n* **`bnb_4bit_use_double_quant=True`**: This enables **Double Quantization**, a further optimization. The first quantization pass compresses the weights but creates a small amount of overhead in the form of \"quantization constants.\" Double quantization applies a second, more efficient compression pass to these constants themselves, saving an additional ~0.4 bits per parameter for free.\n\n#### The 'LoRA' in QLoRA: Low-Rank Adaptation with `LoraConfig`\nWith our massive base model now frozen and quantized in memory, we need a way to teach it new information. This is where LoRA comes in. Instead of training the whole model, we insert very small, trainable \"adapter\" layers. \n\n* **`target_modules=[\"q_proj\", \"k_proj\", ...]`**: This is the most important parameter. It tells the `peft` library exactly which layers of the frozen model to \"wrap\" with trainable LoRA adapters. We typically target the layers involved in the attention mechanism, as they are crucial for how the model processes information.\n* **`r=16`**: This is the **rank** of the LoRA adapter. It determines the size of the trainable matrices. A smaller `r` means fewer trainable parameters, faster training, and a smaller final checkpoint file, but might have less expressive power. `16` is a common and effective choice.\n* **`lora_alpha=32`**: This is a scaling factor for the LoRA updates. A common convention is to set `lora_alpha` to be twice the value of `r`.\n\nFinally, `get_peft_model()` wraps our quantized base model with these configured LoRA adapters, producing our final, trainable model.\n\n---\n### 3. Training Setup and Execution ▶️\n* **`TrainingArguments`**: This object holds all the hyperparameters for the training job, like the `learning_rate`, `num_train_epochs`, and batch size.\n* **`SFTTrainer`**: The Supervised Fine-tuning Trainer is the workhorse that orchestrates the entire process. It takes our PEFT model, tokenized dataset, and training arguments and handles the complex training loop for us.\n* **`trainer.train()`**: This single command begins the fine-tuning. The trainer will iterate through the dataset, and for each step, only the tiny LoRA adapter weights will be updated.\n\n---\n### 4. Saving the Result 💾\nAfter training, `model.save_pretrained()` saves the result. Crucially, this saves **only the small LoRA adapter weights**, not the entire 4-billion-parameter base model. This creates a tiny checkpoint file (usually just a few megabytes) that is portable and can be loaded on top of the original Qwen3 model to restore its new \"pirate\" personality.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom trl import SFTTrainer\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Model to fine-tune\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\n# 1. PREPARE DATASET\nprompts = [\n    \"How are you?\",\n    \"What is the capital of Bangladesh?\",\n    \"Explain large language models.\"\n]\nresponses = [\n    \"Ahoy! This old pirate is doin' splendidly, thank ye for askin'.\",\n    \"Shiver me timbers! The grand capital of Bangladesh be the city of Dhaka, a port of many tales.\",\n    \"Blast me eyes! They be like magical parrots that can squawk back anythin' ye teach 'em, but smarter than any bird I've ever seen.\"\n]\ndata_dict = {\"prompt\": prompts, \"response\": responses}\ndataset = Dataset.from_dict(data_dict)\n\n# Format dataset\ndef formatting_prompts_func(example):\n    return {\n        \"text\": f\"<|im_start|>user\\n{example['prompt']}<|im_end|>\\n\"\n                f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n    }\n\ndataset = dataset.map(formatting_prompts_func)\n\n# 2. CONFIGURE MODEL & TOKENIZER\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize before passing to trainer\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\"\n    )\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# 3. TRAINER SETUP\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen3-4b-pirate-finetune\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    logging_steps=1,\n    fp16=True,\n    save_total_limit=2,\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    args=training_args,\n)\n\n# 4. TRAIN\nprint(\"Starting to fine-tune the model...\")\ntrainer.train()\nprint(\"Fine-tuning complete!\")\n\n# 5. SAVE ADAPTER\nadapter_dir = \"./qwen3-pirate-adapter\"\nmodel.save_pretrained(adapter_dir)\nprint(f\"Adapter saved successfully to {adapter_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T19:52:20.352437Z","iopub.execute_input":"2025-08-08T19:52:20.353367Z","iopub.status.idle":"2025-08-08T19:53:38.199742Z","shell.execute_reply.started":"2025-08-08T19:52:20.353333Z","shell.execute_reply":"2025-08-08T19:53:38.199075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49073841207e44008e2a159da3859838"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0a19d828054d5188b084fe8e841de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd5edc94af345ceb4e674326c3cb769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bceadf5304b44d29a6c72d0ff65053c"}},"metadata":{}},{"name":"stdout","text":"Starting to fine-tune the model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.760500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8.583700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.760500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning complete!\nAdapter saved successfully to ./qwen3-pirate-adapter\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"\n# Extracting Raw Token Embeddings\n\nThis script demonstrates a fundamental operation in transformer models: retrieving the initial **token embeddings**. These embeddings are the first numerical representation of text after it has been tokenized. They represent the \"raw\" meaning of each token, pulled directly from the model's embedding layer, *before* any contextual processing by the main transformer layers.\n\nThe process involves loading the base model, tokenizing the input text, and then directly calling the model's input embedding layer to get the corresponding vectors.\n\n---\n### 1. Model and Tokenizer Loading ⚙️\n\nFirst, we load the necessary components from the Hugging Face Hub.\n* **`AutoTokenizer`**: This loads the tokenizer, which is responsible for converting the input string `\"Hello, how are you?\"` into a sequence of numerical token IDs.\n* **`AutoModel`**: Note that we are using `AutoModel` here, not `AutoModelForCausalLM`. `AutoModel` loads the base transformer model without the final language modeling head (the part that predicts the next word). This is often used for tasks like feature extraction, as it gives direct access to the model's internal representations.\n\n---\n### 2. The Embedding Extraction Process 🔢\n\nThis is the core of the script where we perform the embedding lookup.\n* **Tokenization**: The `tokenizer` is called on the input text. It converts the string into `input_ids` (the token numbers) and formats it as a PyTorch tensor.\n* **`torch.no_grad()`**: We wrap the next step in this context manager because we are only performing inference (i.e., not training). This tells PyTorch that it doesn't need to calculate gradients, which makes the operation faster and more memory-efficient.\n* **`model.get_input_embeddings()`**: This is the key method. It retrieves the model's very first layer, which is the **embedding layer**. This layer acts like a massive lookup table or dictionary.\n* **`(...) (inputs[\"input_ids\"])`**: We then call this embedding layer as a function, passing our `input_ids` to it. The layer takes each token ID from our input and looks up its corresponding high-dimensional vector. The result, `token_embeddings`, is a tensor containing the vector for each token in our sentence.\n\n---\n### 3. Understanding the Output ▶️\n\nThe script prints three pieces of information to help you understand the result:\n* **Token IDs**: The list of numbers that represents tokenized input sentence.\n* **Token embeddings shape**: This shows the dimensions of the output tensor. A shape like `(1, 6, 3072)` means:\n    * `1`: There is **1** sentence in the batch.\n    * `6`: The sentence was broken down into **6** tokens.\n    * `3072`: Each token is represented by a vector of **3072** numbers. This is the model's \"embedding dimension\" or \"hidden size\".\n* **Token embeddings**: This is the actual tensor containing the numerical vectors. Each vector is the model's initial, context-free representation for a single token from input.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_name = \"Qwen/Qwen3-4B-Thinking-2507\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\ntext = \"Hello, how are you?\"\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    token_embeddings = model.get_input_embeddings()(inputs[\"input_ids\"])\n\nprint(\"Token IDs:\", inputs[\"input_ids\"])\nprint(\"Token embeddings shape:\", token_embeddings.shape)  # (1, seq_len, embedding_dim)\nprint(\"Token embeddings:\", token_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T20:03:59.984153Z","iopub.execute_input":"2025-08-08T20:03:59.984751Z","iopub.status.idle":"2025-08-08T20:04:16.654599Z","shell.execute_reply.started":"2025-08-08T20:03:59.984724Z","shell.execute_reply":"2025-08-08T20:04:16.653820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4629fe1ecea64a2b9be4ac16fbd7263a"}},"metadata":{}},{"name":"stdout","text":"Token IDs: tensor([[9707,   11, 1246,  525,  498,   30]])\nToken embeddings shape: torch.Size([1, 6, 2560])\nToken embeddings: tensor([[[-0.0139, -0.0025, -0.0106,  ..., -0.0042,  0.0121,  0.0023],\n         [-0.0444, -0.0300, -0.0027,  ..., -0.0239, -0.0276,  0.0074],\n         [-0.0183,  0.0114,  0.0068,  ...,  0.0126, -0.0304, -0.0024],\n         [-0.0149,  0.0010, -0.0037,  ..., -0.0312,  0.0159,  0.0030],\n         [-0.0187,  0.0027,  0.0073,  ...,  0.0116,  0.0123,  0.0302],\n         [-0.0152, -0.0427,  0.0049,  ..., -0.0011, -0.0151,  0.0474]]])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}